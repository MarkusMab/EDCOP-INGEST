apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "ingest.fullname" . }}-logstash-pipelines
data:
{{- if .Values.logstashConfig.features.syslog.enabled }}
  syslog.conf: |
    input {
      tcp {
        port => 5144
        type => syslog
      }
      udp {
        port => 5144
        type => syslog
      }
    }
    filter {
        grok {
          match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
          add_field => [ "received_at", "%{@timestamp}" ]
          add_field => [ "received_from", "%{host}" ]
        }
        date {
          match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        }
    }
    
    output {
      stdout { codec => rubydebug }
    }
    output {
      elasticsearch {
        hosts => "data-service:9200"
        manage_template => false
        index => "syslog-%{+YYYY.MM.dd}"
        document_type => "%{[@metadata][type]}"
        codec => json
        #user => logstash_internal
        #password => changeme
      }
    }
{{- end }}
{{- if .Values.logstashConfig.features.packetbeat.enabled }}
  packetbeat.conf: |
    input {
      redis {
        host => "ingest-service"
        key => "packetbeat"
        data_type => "list"
        codec => json
        batch_count => {{ .Values.logstashConfig.features.packetbeat.pipeline.batchCount }}
        threads => {{ .Values.logstashConfig.features.packetbeat.pipeline.threads }}
      }
    }
    output {
      elasticsearch {
        hosts => "data-service:9200"
        manage_template => false
        index => "packetbeat-%{+YYYY.MM.dd}"
        document_type => "event"
        codec => json
        #user => logstash_internal
        #password => changeme
      }
    }
{{- end }}
{{- if .Values.logstashConfig.features.bro.enabled }}
  bro.conf: |
    input {
      redis {
        host => "ingest-service"
        key => "bro"
        data_type => "list"
        codec => json
        batch_count => {{  .Values.logstashConfig.features.bro.pipeline.batchCount }}
        threads => {{  .Values.logstashConfig.features.bro.pipeline.threads }}
      }
    }
    filter {
      json {
        source => "message"
      }
      geoip {
      source => "id.resp_h"
      target => "resp_geoip"
      add_field => [ "resp_location", "%{[resp_geoip][longitude]}" ]
      add_field => [ "resp_location", "%{[resp_geoip][latitude]}"  ]
      }
      mutate {
        convert => [ "resp_location", "float" ]
        replace => { "type" => "bro" }
        rename => { "id" => "bro_id" }
        #need to rename the following because Elastic assumes they are objects and fails
        rename => { "id.resp_p" => "bro_id_resp_p" }
        rename => { "id.resp_h" => "bro_id_resp_h" }
        rename => { "id.orig_p" => "bro_id_orig_p" }
        rename => { "id.orig_h" => "bro_id_orig_h" }
      }
    } 
 
    output {
      elasticsearch {
        hosts => "data-service:9200"
        manage_template => false
        index => "bro-%{+YYYY.MM.dd}"
        document_type => "event"
        codec => json
        #user => logstash_internal
        #password => changeme
      }
    }
{{- end }}
{{- if .Values.logstashConfig.features.suricata.enabled }}
  suricata.conf: |
    input {
      redis {
        host => "ingest-service"
        key => "suricata"
        data_type => "list"
        codec => json
        batch_count => {{  .Values.logstashConfig.features.suricata.pipeline.batchCount }}
        threads => {{  .Values.logstashConfig.features.suricata.pipeline.threads }}
      }
    }
    output {
      elasticsearch {
        hosts => "data-service:9200"
        manage_template => false
        index => "suricata-%{+YYYY.MM.dd}"
        document_type => "event"
        codec => json
        #user => logstash_internal
        #password => changeme
      }
    }
{{- end }}